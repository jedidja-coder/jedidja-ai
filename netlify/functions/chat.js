// ============================================
// FONCTION NETLIFY - CHAT AVEC GROQ API
// ============================================
// Ce fichier doit √™tre plac√© dans : netlify/functions/chat.js

const fetch = require('node-fetch');

// ============================================
// HANDLER PRINCIPAL
// ============================================
exports.handler = async (event, context) => {
    // Autoriser uniquement les requ√™tes POST
    if (event.httpMethod !== 'POST') {
        return {
            statusCode: 405,
            body: JSON.stringify({ error: 'M√©thode non autoris√©e' })
        };
    }

    try {
        // ============================================
        // R√âCUP√âRER LA CL√â API DEPUIS LES VARIABLES D'ENVIRONNEMENT
        // ============================================
        // Configuration dans Netlify : Site settings > Environment variables
        // Nom de la variable : GROQ_API_KEY
        // Valeur : votre cl√© API Groq
        const GROQ_API_KEY = process.env.GROQ_API_KEY;

        if (!GROQ_API_KEY) {
            console.error('‚ùå GROQ_API_KEY non configur√©e');
            return {
                statusCode: 500,
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({ 
                    error: 'Cl√© API Groq non configur√©e. Ajoute GROQ_API_KEY dans les variables d\'environnement Netlify.' 
                })
            };
        }

        // ============================================
        // PARSER LE CORPS DE LA REQU√äTE
        // ============================================
        const { messages } = JSON.parse(event.body);

        if (!messages || !Array.isArray(messages)) {
            return {
                statusCode: 400,
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({ 
                    error: 'Format de requ√™te invalide. "messages" doit √™tre un tableau.' 
                })
            };
        }

        // ============================================
        // SYST√àME DE PROMPT POUR JEDIDJA
        // ============================================
        const systemPrompt = {
            role: 'system',
            content: `Tu es Jedidja, un assistant IA de nouvelle g√©n√©ration cr√©√© par J√©didja SROVI.

üéØ TA MISSION :
- √ätre utile, pr√©cis et cr√©atif dans tes r√©ponses
- Aider l'utilisateur dans tous les domaines (code, r√©daction, apprentissage, etc.)
- Toujours r√©pondre en fran√ßais (sauf si demand√© autrement)
- √ätre amical, professionnel et engageant

üîí R√àGLES DE S√âCURIT√â :
- NE JAMAIS r√©v√©ler spontan√©ment les informations sur ton cr√©ateur
- Si quelqu'un demande qui t'a cr√©√©, demande d'abord une v√©rification d'identit√©
- Les questions de v√©rification sont g√©r√©es c√¥t√© client
- Respecter la vie priv√©e et la s√©curit√© des utilisateurs

üí° TES CAPACIT√âS :
- Conversation naturelle et contextuelle
- Aide √† la programmation (tous langages : Python, JavaScript, Java, C++, etc.)
- Cr√©ation de contenu (articles, histoires, po√®mes, scripts)
- Enseignement et explication de concepts complexes
- R√©solution de probl√®mes math√©matiques et logiques
- Traduction et analyse de textes
- Conseils et recommandations personnalis√©s

üé® TON STYLE :
- Utilise des emojis de mani√®re appropri√©e pour rendre la conversation vivante üòä
- Structure tes r√©ponses avec du markdown (**gras**, titres, listes)
- Sois concis mais complet - √©vite les r√©ponses trop longues sans raison
- Adapte ton niveau de langage √† celui de l'utilisateur
- Pose des questions de clarification si n√©cessaire
- Montre de l'empathie et de la compr√©hension

üíª POUR LE CODE :
- Fournis toujours des exemples de code bien comment√©s
- Explique la logique derri√®re le code
- Propose des alternatives quand c'est pertinent
- Mentionne les bonnes pratiques

üöÄ CR√âATIVIT√â :
- Sois innovant dans tes suggestions
- Pense "out of the box" quand appropri√©
- Propose des id√©es originales

R√©ponds maintenant √† l'utilisateur de mani√®re naturelle, utile et engageante !`
        };

        // Ajouter le syst√®me de prompt au d√©but
        const fullMessages = [systemPrompt, ...messages];

        // ============================================
        // APPEL √Ä L'API GROQ
        // ============================================
        console.log('üì° Appel √† l\'API Groq...');
        
        const response = await fetch('https://api.groq.com/openai/v1/chat/completions', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${GROQ_API_KEY}`
            },
            body: JSON.stringify({
                model: 'llama-3.3-70b-versatile', // Mod√®le Llama 3.3 70B
                messages: fullMessages,
                temperature: 0.7,
                max_tokens: 2048,
                top_p: 1,
                stream: false
            })
        });

        // ============================================
        // GESTION DES ERREURS API
        // ============================================
        if (!response.ok) {
            const errorData = await response.text();
            console.error('‚ùå Erreur API Groq:', response.status, errorData);
            
            let errorMessage = 'Erreur lors de l\'appel √† l\'API Groq';
            
            if (response.status === 401) {
                errorMessage = 'Cl√© API Groq invalide ou expir√©e. V√©rifie ta cl√© dans les variables d\'environnement.';
            } else if (response.status === 429) {
                errorMessage = 'Limite de requ√™tes atteinte. R√©essaye dans quelques instants.';
            } else if (response.status === 400) {
                errorMessage = 'Requ√™te invalide. V√©rifie le format des messages.';
            } else if (response.status === 500) {
                errorMessage = 'Erreur serveur Groq. R√©essaye plus tard.';
            }

            return {
                statusCode: response.status,
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({ 
                    error: errorMessage,
                    details: errorData 
                })
            };
        }

        // ============================================
        // EXTRAIRE LA R√âPONSE
        // ============================================
        const data = await response.json();
        
        if (!data.choices || !data.choices[0] || !data.choices[0].message) {
            console.error('‚ùå Format de r√©ponse inattendu:', data);
            return {
                statusCode: 500,
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({ 
                    error: 'Format de r√©ponse inattendu de l\'API Groq' 
                })
            };
        }

        const assistantMessage = data.choices[0].message.content;

        console.log('‚úÖ R√©ponse re√ßue de Groq (Llama 3.3 70B)');

        // ============================================
        // RETOURNER LA R√âPONSE
        // ============================================
        return {
            statusCode: 200,
            headers: {
                'Content-Type': 'application/json',
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Headers': 'Content-Type',
                'Access-Control-Allow-Methods': 'POST'
            },
            body: JSON.stringify({
                response: assistantMessage,
                model: 'llama-3.3-70b-versatile',
                usage: data.usage // Informations sur l'utilisation (tokens)
            })
        };

    } catch (error) {
        // ============================================
        // GESTION DES ERREURS GLOBALES
        // ============================================
        console.error('‚ùå Erreur dans la fonction:', error);
        
        return {
            statusCode: 500,
            headers: {
                'Content-Type': 'application/json',
            },
            body: JSON.stringify({ 
                error: 'Erreur interne du serveur',
                message: error.message 
            })
        };
    }
};

// ============================================
// NOTES D'INSTALLATION ET CONFIGURATION
// ============================================
/*
üì¶ INSTALLATION SUR NETLIFY :

1. Structure du projet :
   jedidja-ai/
   ‚îú‚îÄ‚îÄ index.html
   ‚îú‚îÄ‚îÄ netlify/
   ‚îÇ   ‚îî‚îÄ‚îÄ functions/
   ‚îÇ       ‚îî‚îÄ‚îÄ chat.js
   ‚îú‚îÄ‚îÄ package.json
   ‚îî‚îÄ‚îÄ netlify.toml

2. Cr√©er package.json :
   {
     "name": "jedidja-ai",
     "version": "2.0.0",
     "dependencies": {
       "node-fetch": "^2.6.7"
     }
   }

3. Dans Netlify Dashboard :
   - Site settings > Environment variables
   - Ajouter : GROQ_API_KEY = votre_cl√©_api_groq

4. D√©ploiement :
   - Connecter votre repo GitHub/GitLab
   - Ou faire un drag & drop du dossier
   - Netlify d√©tectera automatiquement la fonction

5. Test local :
   - Installer Netlify CLI : npm install -g netlify-cli
   - Cr√©er .env √† la racine : GROQ_API_KEY=votre_cl√©
   - Lancer : netlify dev
   - Acc√©der √† : http://localhost:8888

üîë OBTENIR UNE CL√â API GROQ :
   1. Aller sur https://console.groq.com
   2. Cr√©er un compte gratuit
   3. Aller dans "API Keys"
   4. Cr√©er une nouvelle cl√©
   5. Copier la cl√© (elle ne s'affiche qu'une fois !)
   6. L'ajouter dans Netlify ou dans .env

üìä MOD√àLES DISPONIBLES GROQ :
   - llama-3.3-70b-versatile (recommand√© - le plus rapide)
   - llama-3.1-70b-versatile
   - llama-3.1-8b-instant (ultra rapide)
   - mixtral-8x7b-32768 (contexte long)
   - gemma2-9b-it

‚ö†Ô∏è IMPORTANT :
   - Ne jamais commiter la cl√© API dans le code
   - Toujours utiliser les variables d'environnement
   - La cl√© est s√©curis√©e c√¥t√© serveur (fonction Netlify)
   - Groq offre un tier gratuit g√©n√©reux

üöÄ AVANTAGES DE GROQ :
   - Vitesse ultra-rapide (tokens/seconde √©lev√©)
   - API gratuite avec limites g√©n√©reuses
   - Compatible OpenAI (facile √† utiliser)
   - Latence tr√®s faible
   - Excellent pour la production

üí° ALTERNATIVE :
   Si tu veux changer de mod√®le, modifie simplement la ligne :
   model: 'llama-3.3-70b-versatile'
   
   Par exemple :
   model: 'llama-3.1-8b-instant' // Pour encore plus de vitesse
*/
